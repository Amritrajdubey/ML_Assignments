{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44792902",
   "metadata": {},
   "source": [
    "1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ceace3",
   "metadata": {},
   "source": [
    "Target function are functions that we aim to opetimize or approximate, often within the context of ML , Optimization or decision making process. We can use numerous target functions depending on our use case , Ex - Loss function for the calculation of variation b/w the prdeicted and actual figures , Accuracy metrix , R - sq or Adj R-sq to calculate the Model accuarcy which again depend on our problem that can be of Regression / Classification. \n",
    "\n",
    "- Real life example using above Target functions :\n",
    "Imagine we want to predict House price using a Regression model which depends on Size of house , Nos of Bedroom , Locality , Age of House , Appartment / Flat:\n",
    "\n",
    "Considering above case target function to determine the Price can be - f(Size of House,Nos of Bedroom,Locality,Age,Nature of building) = Price\n",
    "Now how well model is ,can be assessed by how well it fit or predict the actual price compared with the available actual data of House price. Now let's conside below Loss function and Accuracy matrix for further explanation :\n",
    "- Loss function : Using loss function we can calculate the difference b/w the predicted and actual value.Common loss function we can use for above regression function is MSE , MAE. Consider by using any of Loss function predicted value comes to be 60 Lakh for a flat but the actual figure is 65 Lakh. Here difference is of 5 Lakhs in the predicted value, Now how well our model is predicting depend on less the differene be , Less the difference better Loss function is performing . \n",
    "\n",
    "- R-sq / Adj R-sq : We use Adj R-sq for House price Model prediction accuracy calculation bcz R-sq prediction value increase with increase in nos of constrains even if they must not effect the House price so using Adj R-sq can be helpfull. Thus we can get how well the model explain the Target function using Accuracy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cecc02",
   "metadata": {},
   "source": [
    "2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4e304",
   "metadata": {},
   "source": [
    "Predictive models are those models used to make future prediction based on input historical data. These models tries to identify pattern or relation within the past data and based on that it makes prediction of future or unseen data.\n",
    "\n",
    "- How it works :\n",
    "We build model based on the problem statement and further train it on Training data which has leveled data where both input features (independent variable) and corresponding output (dependent variables) are known. Further model learn the patterns in the data which depend on the algorithm we use like Linear Regression, Decession Tree , XGBoost etc. Then we predict the output based on the learnt model by providing unseen input data and further do the evaluation to calculate the accuracy of model by using various metrics like Accuracy , Precision , MAE , R-sq etc.\n",
    "\n",
    "Descriptive models are different from predictive models in which we basically focus on understanding , summarizing , describing relationship in data. We don't make prediction of the unseen data as doing in predictive models rather we focus on collecting insights of the data.\n",
    "\n",
    "- How it works :\n",
    "We indentify patterns , insights , trends within data and we also summarize the data on basis of clustering them , grouping to figure out insights well. Unlike predictive models we don't do any prediction rather focus on understanding what's happening within data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690258a",
   "metadata": {},
   "source": [
    "3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d791869",
   "metadata": {},
   "source": [
    "Assessing the efficiency of a classification model is critical in figuring out how well the model is performing and prdeicting the catrgorical output.Several methods and metrics are used for the evaluation of classification method. Following are some of methods for calculation :\n",
    "\n",
    "- Confusion matrix : It's key tool to evaluate performance of classification model. It's table that compare actual (true) lables with the predicted labels.\n",
    "\n",
    "TP - True Positive , TN - True Negative , FP - False Positive , FN - False Negative\n",
    "\n",
    "Based on confusion matrix several important metrices can be derived :\n",
    "Accuracy : It's most accurate measuring of how actual the model predicts, It's ratio of correct predicted instance devided by total instances ( Both negative and positive ). ( TP + TN )/ ( TP + TN + FP + FN )\n",
    "\n",
    "Precision : It measure how many of predicted positive cases were actual positive. It's ratio of TP/(TP + FP ).\n",
    "\n",
    "- AUC - ROC Curve : ROC ( Receiver Operating Characteristics ): it's curve plot b/w true positive against the false positive rate for different classification thresholds. AUC ( Area Under Curve ) is a single value that summarize the ROC curve , more the value tends toward 1 better the model accuracy is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18f3d0",
   "metadata": {},
   "source": [
    "4. \n",
    "      i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
    "     ii. What does it mean to overfit? When is it going to happen?\n",
    "    iii. In the sense of model fitting, explain the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c99454",
   "metadata": {},
   "source": [
    "Underfitting :\n",
    "Underfitting occurs when model is too simple or bad to capture any trend or pattern within the training dataset and perform bad with both training data and unseen data bcz it fails to learn the relationship b/w input features and target output.\n",
    "- Reasons for underfitting :\n",
    "Model is not complex enough to learn from data. Ex - Using classification based model for Regression problem.\n",
    "Lack of training data , lack of relevant features or relevant information.\n",
    "\n",
    "Overfutting :\n",
    "Overfitting occurs when model is too complex and learns not only underlying pattern in data but also noise and irrelevant details. Model fit well with the training data but fails to fit well with test or unseen data.\n",
    "- Reason for Overfitting :\n",
    "Model is overly complexed with too many parameters . Ex - using very deep decision tree with much tree depth can led to overfitting.\n",
    "Small training data and lack of features can let model not learn model prediciton capability well and can led to overfitting.\n",
    "\n",
    "Bias-variance trade-off :\n",
    "It's fundamental concept in ML that explains the balance b/w overfitting and underfitting. It deals with how well a model generalizes to unseen data based on Bias and Variance.\n",
    "- Bias :\n",
    "Bias refer to the error introduced by the model due to oversimplification of data. A high bias model make strong assumptition about the data and most likely is to underfit. It fails to capture the complexity of data , leading to poor predictions on both training and test data. \n",
    "High bias led to underfitting.\n",
    "- Variance :\n",
    "Variance refer to the model sensitivity to small fluctuations or noise in the training data.A high variance model is very flexible and tries to fit every details while training model, including the noise leading to overfitting.It perform well with training data but poorly with new data.\n",
    "\n",
    "- Trade-off :\n",
    "In ML we insist on finding model that balance bias and variance, minimizing overall error. More biased model let to underfit while model with high varience can led to overfitting.\n",
    "ways to manage bias-variance trade-off :\n",
    "- Increase model complexity.\n",
    "- Cross validation evaluation technique for model performance verification.\n",
    "- Use of L1 & L2 regularization techinques to prevent model from overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5e3cb",
   "metadata": {},
   "source": [
    "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab6112",
   "metadata": {},
   "source": [
    "Yes , It's possible to boost efficiency of a learning model. We can use various methods to boost model efficiency. Following are some methods :\n",
    "- Data Related improvements :\n",
    "Increasing amount of training data.\n",
    "Improve data quality. Handling missing data , Outliers removals , Normalization/standardization of data.\n",
    "Features engineering.\n",
    "\n",
    "- Model related improvements :\n",
    "Choosing right models.\n",
    "Regularization ( L1 & L2 )\n",
    "Ensemble learning\n",
    "Use of Grid search/ Bayesian optimization.\n",
    "\n",
    "- ALgorithm related improvements :\n",
    "Use of more advanced model as some models performs well with certain tasks due to there inherent ability to capture complex relation within data. We can use Gradient boosting techniques , Neural network architecture.\n",
    "\n",
    "- Cross validation :\n",
    "Cross validation can help us deivide datasets into multiple subsets and trainig the data on them thus reducing risk of over and underfitting .\n",
    "\n",
    "- Transfer learning : \n",
    "We can leverage pre trained architecture for transfer learning when labled data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222693e6",
   "metadata": {},
   "source": [
    "6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0acbd9",
   "metadata": {},
   "source": [
    "Most common indicators to rate unsupervised learning model's success :\n",
    "- Clustering models : Clustering is most common spproach in unsupervised learning where we aim to group similar data points togethers to form clusters. To measure success rate of clustering we can use Silhouette score, Inertia.\n",
    "\n",
    "Silhouette score : It's score varries from -1 to 1 . where -1 indicates that sample might have landed in wrong cluster , 1 indicates sample is far away from neighboring cluster and 0 means it's very close or on the boundary b/w clusters.\n",
    "\n",
    "Inertia ( WIthin cluster sum of square ): Inertia measure how tightly the clusters are packed and is measured by consodering sum of sq distance b/w data points and there corresponding cluster centroid. Lower inetria means clusters are tightly packed and better cluster. \n",
    "\n",
    "Dimensionality reduction : It aim to reduce the nos of feature at same time retaining as much varience and structure in data at same time. We can Evaluation it by using PCA.\n",
    "\n",
    "Anamoly detection: Anamoly detection usually refer to identification of outliers , unusual trends within data.Success indicators for anamoly detection usually depend on model ability to indentify Normal and Anamolous data. We can use Precision , Recall , AUC-ROC curve to measure the accuracy.\n",
    "\n",
    "General indicators to measure success of Unsupervised learning :\n",
    "- Interpretability : A good model should yield results that are meaningfull and interpretable. Discovered patterns or cluster could make sense in context to data and its domain.\n",
    "- Stability : Model must be stable with different data points and it must find similar patterns or clusters across different runs , especially during methods like K- means clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d73f2c",
   "metadata": {},
   "source": [
    "7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2efe5",
   "metadata": {},
   "source": [
    "In ML classification models are designed for Categorical data and regression models for numcerical data but with certin modification in the model we can use them vice versa.\n",
    "\n",
    "- Using Regression model for Categorical data :\n",
    "Regression model are used to predict continous data but can be modified to handle categorical data, such as with ordinal categories :\n",
    "Ordinal encoding :\n",
    "In case of natural order we can assign numerical lables to them and use the regression model for them.These kind of models are called ordinal regression.\n",
    "Regression with one hot encoding :\n",
    "If there is no order in the target categorical variable then we can directly use regression model for categorical data but rather we can use One hot encoding to the target categories to transform target variable into multi target regression problem.\n",
    "\n",
    "- Using Categorical model for continous numerical data :\n",
    "Categorical model is used to predict categorical data but we can use it for continous numerical data prediction also. Such as by using Bin for numerical data in categories :\n",
    "Bin the numerical data into Categories : \n",
    "If we have continous numerical target variable we can bin it into categories and then use classification data.\n",
    "Ordinal classification :\n",
    "If target numerical value has an inherit order we can treat it as Ordinal classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84ddca",
   "metadata": {},
   "source": [
    "8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273f6bb",
   "metadata": {},
   "source": [
    "Predictive modeling for numerical values are called regression modeling in which we aim to predict continous numerical value based on set of input fetures vs in case of Categorical predictive modeling our aim is to predict discrete levels . We use classification based model for them and difference in both lies in the type of target variables to predict and the models bwing used.\n",
    "\n",
    "Predictive models for Regression ( Numerical ) problems :\n",
    "In Regression models target variable is continuous and take any nos with a specific range. We can use these models for Heigh , Weight , Mass , Income , Flight Price etc prediction.\n",
    "\n",
    "Various Regression models :\n",
    "- Linerar regression models\n",
    "- Polynomial regression\n",
    "- Decesio Tree\n",
    "- XG Boost \n",
    "- SVM \n",
    "- Neural network.\n",
    "\n",
    "General Pipeline involve :\n",
    "Data collection -- Data Cleaning and EDA -- Testing Data and it's accuracy by using above models , whichever finds best accuracy by using Model eveluation metrics ( MAE/ MAE/ R-sq / Adj R-sq ) -- Model validation and Tunning -- Model deployment.\n",
    "\n",
    "Predictive models for Categorical ( DIscrete/Labled ) problems :\n",
    "Main difference in the models is in the target variable variable we are supposed to define , In case of Categorical model we predict discrete variable or labled data which can be binary or multi class.\n",
    "\n",
    "Varous Classification model :\n",
    "- Logistic regression\n",
    "- Decision tree\n",
    "- SVM\n",
    "- Neural network.\n",
    "\n",
    "Pipeline is same for both Regression and Classification model.\n",
    "\n",
    "Difference in Classification model when compared to regression model :\n",
    "- Target variable : Target variable in case of Categorical problem is discrete , labled while in Regression it's continous number.\n",
    "- Algorithms : We use different ALgo for both kind of problems.\n",
    "- Evaluation matrics : In case of Regression problems evaluation metrics used to test models are ( MAE , MSE , R-sq , RMSE ) for Classification problems metrics used are Accuracy , F1 , Precision , Recall , ROU - AUC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe59c9",
   "metadata": {},
   "source": [
    "9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n",
    "         i. Accurate estimates – 15 cancerous, 75 benign\n",
    "         ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0aea0",
   "metadata": {},
   "source": [
    "TP : 15 , TN : 75 , FP : 7 , FN : 3\n",
    "\n",
    "Error Rate : ( FP+FN ) / ( TP+TN+FP+FN ) = 10/100 = .10 or 10%\n",
    "\n",
    "Kappa Value : It's comparision of Model accuracy with accuracy with random chance. \n",
    "                  Model Accuracy = ( TP+TN )/ Total Predictions = 90/100 = .9 \n",
    "                  Expected Aggrement by chance : ( (( TP+FP ) x ( TP+FN )) + (( TN+FP ) x ( TN+FN )) )/ Total predictions -sq\n",
    "                  \n",
    "                  = ( ((15+7) x (15+3))  +  (( 75+7 ) x ( 75+3)) )/ 100*100\n",
    "                  = ( 396 + 6396 )/10000\n",
    "                  = .676\n",
    "                  \n",
    "               Kappa value = 0.90 - .676 / ( 1 - .676 ) = 0.688\n",
    "               \n",
    "Sensitivity ( Recall ) : TP / ( TP + FN ) = 15 / ( 15+3 ) = 0.833 or 83%\n",
    "\n",
    "Precision : TP / ( TP+FP ) = 15/( 15+7 ) = .682 or 68.2%\n",
    "\n",
    "F Measure : 2 ( Precesion x Sensitivity ) / ( Precesion + Sensitivity ) = 2 ( .682 x .833 ) / ( .682 + .833 ) = .75 or 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80bb90",
   "metadata": {},
   "source": [
    "10. Make quick notes on:\n",
    "         1. The process of holding out\n",
    "         2. Cross-validation by tenfold\n",
    "         3. Adjusting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731264ce",
   "metadata": {},
   "source": [
    "- The Process of holding out :\n",
    "It's simple way to evaluate the model by splitting the datasets into two parts : Training and Testing data.\n",
    "\n",
    "Steps : Split the datasets into two sets in 70%-80% ratio for training data and 30%-20% for testing model. -- Trsin the model on training data and test them on unseen testing data. It's easy to implement but result depends on the random data split.\n",
    "\n",
    "- Cross validation ( Tenfold ) : \n",
    "It's more robust model evaluation technique that mitigates overfitting by dividing the data into 10 subsets and further training the model on 9 data subsets and testing the models a subset. We repeat same process 10 times by using different fold for validation. Then we avearge the overall result of each time to have final validation result.\n",
    "\n",
    "- Adjust the Parameters :\n",
    "These involves the process of optimizing the model's hyperparameters for model inprovements.\n",
    "We can choose hyperparameters ( Learning rate , Decision tree depth , No's of tree etc ) using Grid search cv , Bayseian optimization , Random search etc. Using Hyperparameters we led model learn complex patterns leading to better model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30883a81",
   "metadata": {},
   "source": [
    "11. Define the following terms: \n",
    "         1. Purity vs. Silhouette width\n",
    "         2. Boosting vs. Bagging\n",
    "         3. The eager learner vs. the lazy learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875a7de",
   "metadata": {},
   "source": [
    "- Purity vs Silhouette width :\n",
    "Purity : It's a measure used to evaluate the quality of clusters in clustering algorithms. It measures how well the clusters corresponds to the ground truth levels. It varies b/w 0 to 1 where 1 means all data points are within a clusters belonging to same class and 0 represents all data points belongs to different classes.\n",
    "\n",
    "Silhouette width :It's metric used to measure the quality of clustering based on both seperation and cohesion. It measure average distance b/w within and iner cluster distance. it varies from -1 to +1 where +1 represent a well cluster , 0 indicates points on the boundary b/w clusters and negative value represent points in different cluster.\n",
    "\n",
    "- Boosting vs Bagging :\n",
    "Boosting : It's ensembled technique that sequentially trains model to focus on the mistake of the previous models. Each sequentially models give more weight to misclassified data points making going forward model to learn accordingly. Models are trained one after another with each model correcting the error of previous model and final prediction is weighted sum of all individual models. Models used are XGBoost , AdaBoost , Gradient Boost.\n",
    "\n",
    "Bagging : It's also ensemble techniques that trains multiple models in parallel on different subsets of data and average there prediction to have final outcome.Our aim using such learning model is to reduce variance by creating diverse model being less prone to noisy data. Models used are Random forest.\n",
    "\n",
    "- The Eager learner vs The Lazy learner :\n",
    "Eager learner : Ml models that build a generalized model from taining data before receiving any queries. Once training phase is completed we can start making predictions with new data. Model simply learn the patterns and parameters from the training data in advance and prediction is done quickly as soon data is given. It's fast but not accurate when data keep on changing , Noise occurs. Models used are : Decision tree , Linear reg , SVM\n",
    "\n",
    "Lazy learner :ML models that does'nt build explicit model during training instead it store the training data and only do computation when quired with new data. Whenever a query arrives model search through the stored data to find most relevant instances to make prediction . Models used : K- Nearest neighbour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be207ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163dfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
